<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>Context Optimization in Agentic AI</title>
<style>
  body {
    font-family: 'adobe-clean', 'Source Sans Pro', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
    font-size: 20px;
    line-height: 1.5;
    width: 800px;
    height: 600px;
    margin: 0;
    padding: 0;
    position: absolute;
    top: 50%;
    left: 50%;
    margin-left: -400px;
    margin-top: -300px;
    overflow: hidden;
    background: #ffffff;
    color: #2c2c2c;
  }
  
  section {
    position: absolute;
    width: 100%;
    height: 100%;
    padding: 40px 60px;
    box-sizing: border-box;
    background: #ffffff;
    overflow: auto;
    transition: left 0.4s ease;
  }
  
  section { left: -150%; }
  section[aria-selected] { left: 0; }
  section[aria-selected] ~ section { left: 150%; }
  
  h1 {
    font-size: 48px;
    color: #eb1000;
    margin-bottom: 20px;
    font-weight: 700;
    letter-spacing: -0.5px;
  }
  
  h2 {
    font-size: 40px;
    color: #eb1000;
    margin-bottom: 20px;
    font-weight: 700;
    padding-bottom: 8px;
    border-bottom: 3px solid #eb1000;
  }
  
  h3 {
    font-size: 28px;
    color: #505050;
    margin: 20px 0 10px 0;
    font-weight: 600;
  }
  
  ul, ol {
    margin: 15px 0 15px 30px;
  }
  
  li {
    margin: 8px 0;
    font-size: 22px;
    color: #2c2c2c;
  }
  
  code {
    background: #f5f5f5;
    padding: 2px 8px;
    border-radius: 3px;
    font-size: 18px;
    color: #eb1000;
    font-family: 'Source Code Pro', 'Courier New', monospace;
    border: 1px solid #e0e0e0;
  }
  
  pre {
    background: #1e2837;
    color: #d4d4d4;
    padding: 20px;
    border-radius: 8px;
    overflow-x: auto;
    font-size: 14px;
    line-height: 1.6;
    margin: 15px 0;
    border-left: 4px solid #eb1000;
    box-shadow: 0 2px 8px rgba(0,0,0,0.15);
  }
  
  pre code {
    background: transparent;
    color: #d4d4d4;
    font-size: 14px;
    padding: 0;
    border: none;
    font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
  }
  
  /* Syntax highlighting for code blocks */
  pre .comment { color: #6a9955; }
  pre .keyword { color: #c586c0; }
  pre .string { color: #ce9178; }
  pre .function { color: #dcdcaa; }
  pre .variable { color: #9cdcfe; }
  pre .number { color: #b5cea8; }
  pre .operator { color: #d4d4d4; }
  pre .class { color: #4ec9b0; }
  pre .property { color: #9cdcfe; }
  
  .two-col {
    display: grid;
    grid-template-columns: 1fr 1fr;
    gap: 30px;
    margin: 20px 0;
  }
  
  .box {
    padding: 15px;
    border-radius: 4px;
    margin: 15px 0;
    border-left: 4px solid;
  }
  
  .info {
    background: #e3f2fd;
    border-left-color: #1976d2;
    color: #0d47a1;
  }
  
  .warning {
    background: #fff3e0;
    border-left-color: #f57c00;
    color: #e65100;
  }
  
  .success {
    background: #e8f5e9;
    border-left-color: #388e3c;
    color: #1b5e20;
  }
  
  .pros { 
    color: #00875a;
    font-weight: 700;
  }
  
  .cons { 
    color: #eb1000;
    font-weight: 700;
  }
  
  table {
    width: 100%;
    border-collapse: collapse;
    margin: 15px 0;
    font-size: 18px;
    box-shadow: 0 1px 3px rgba(0,0,0,0.1);
  }
  
  th, td {
    padding: 12px 15px;
    text-align: left;
    border-bottom: 1px solid #e0e0e0;
  }
  
  th {
    background: #eb1000;
    color: white;
    font-weight: 700;
    text-transform: uppercase;
    font-size: 16px;
    letter-spacing: 0.5px;
  }
  
  tr:nth-child(even) {
    background: #fafafa;
  }
  
  tr:hover {
    background: #f5f5f5;
  }
  
  .center {
    text-align: center;
  }
  
  strong {
    color: #2c2c2c;
    font-weight: 700;
  }
  
  #progress {
    position: fixed;
    bottom: 0;
    left: 0;
    height: 4px;
    background: #eb1000;
    transition: width 0.3s ease;
    box-shadow: 0 0 10px rgba(235, 16, 0, 0.5);
  }
  
  #slide-number {
    position: fixed;
    bottom: 15px;
    right: 20px;
    font-size: 14px;
    color: #eb1000;
    font-weight: 600;
    background: rgba(255, 255, 255, 0.9);
    padding: 5px 12px;
    border-radius: 20px;
    box-shadow: 0 2px 5px rgba(0,0,0,0.1);
  }
  
</style>
</head>
<body>

<section>
  <h1 class="center">Context Optimization in Agentic AI</h1>
  <p class="center" style="font-size: 32px; margin-top: 40px;">Building Efficient Multi-Agent Systems</p>
  <p class="center" style="font-size: 24px; margin-top: 30px; color: #7f8c8d;">
    TypeScript ‚Ä¢ LangChain ‚Ä¢ LangGraph
  </p>
  <p class="center" style="margin-top: 80px; font-size: 28px; font-weight: 600; color: #2c3e50;">
    Presented by: Abani Ranjan Behera
  </p>
  <p class="center" style="font-size: 20px; margin-top: 10px; color: #7f8c8d;">India Tech Council</p>
</section>
<section>
  <h2>About Me</h2>
  
  <div style="margin-top: 20px;">
    <h3 style="color: #2c2c2c;">Abani Ranjan Behera</h3>
    <p style="font-size: 20px; margin-top: 20px; line-height: 1.8;">
      Senior Solution Architect - Adobe.com team
    </p>
  </div>

  <div style="margin-top: 25px;">
    <h3>Current Project: Stream</h3>
    <div class="box info" style="font-size: 18px;">
      <strong>Project Stream</strong> is an agentic AI platform designed to automate 
      content creation workflows for Adobe's digital properties.
    </div>
    
    <ul style="font-size: 18px; margin-top: 20px;">
      <li><strong>Multi-Agent Orchestration</strong>: Coordinating specialized agents for content generation</li>
      <li><strong>Context Engineering</strong>: Implementing advanced optimization techniques at scale</li>
      <li><strong>Production AI</strong>: Building reliable, cost-effective agentic systems</li>
    </ul>
  </div>

  <p style="margin-top: 25px; font-size: 20px; color: #7f8c8d;">
    üí° Today's session: Lessons learned from building Stream
  </p>
</section>
<section>
  <h2>Agenda</h2>
  <ol style="font-size: 24px; line-height: 1.8;">
    <li>Understanding LLMs & AI Agents</li>
    <li>What is Context & Why It Matters</li>
    <li>Context Pollution Problem</li>
    <li>Multi-Agent Context Challenges</li>
    <li><strong>6 Context Optimization Techniques:</strong>
      <ul style="font-size: 22px; margin-top: 10px;">
        <li>Pruning</li>
        <li>Summarization</li>
        <li>Context Offloading</li>
        <li>Tool Offloading</li>
        <li>Semantic Editing</li>
        <li>Hierarchical Context</li>
      </ul>
    </li>
    <li>Model Context Protocol (MCP) Deep Dive</li>
    <li>Multi-Agent Patterns & Production Examples</li>
    <li>Best Practices & Common Pitfalls</li>
  </ol>
</section>

<section>
  <h2>LLMs & AI Agents: Foundation</h2>
  <h3>Large Language Models (LLMs)</h3>
  <ul>
    <li>Process and generate text based on input <strong>context</strong></li>
    <li>Limited context window (8K-200K tokens)</li>
    <li>Stateless: no memory between interactions</li>
  </ul>
  <h3>AI Agents</h3>
  <ul>
    <li>LLM + Tools + Memory + Reasoning</li>
    <li>Can execute actions, maintain state, plan</li>
    <li>Context is their "working memory"</li>
  </ul>
</section>

<section>
  <h2>What is Context?</h2>
  <div class="box info">
    <strong>Context = All information provided to the LLM to generate a response</strong>
    <p style="margin-top: 10px; font-size: 20px;">Think of it as the LLM's "working memory" - everything the model can see and use to understand your request and produce relevant output.</p>
  </div>
  <h3>Context Components:</h3>
  <ol style="font-size: 22px;">
    <li><strong>System Prompt</strong>: Instructions & role definition</li>
    <li><strong>Conversation History</strong>: Past messages</li>
    <li><strong>Tool Results</strong>: Function call outputs</li>
    <li><strong>Retrieved Documents</strong>: RAG, knowledge bases</li>
    <li><strong>Agent State</strong>: Variables, metadata</li>
  </ol>
  <p style="margin-top: 20px; font-size: 20px;">üí° Each token costs money and processing time</p>
</section>

<section>
  <h2>Why Context Matters</h2>
  <div class="two-col">
    <div>
      <h3 class="pros">‚úì Good Context</h3>
      <ul style="font-size: 20px;">
        <li>Accurate responses</li>
        <li>Lower latency</li>
        <li>Reduced costs</li>
        <li>Better reasoning</li>
        <li>Scalable agents</li>
      </ul>
    </div>
    <div>
      <h3 class="cons">‚úó Poor Context</h3>
      <ul style="font-size: 20px;">
        <li>Token limit errors</li>
        <li>Slow responses</li>
        <li>High API costs</li>
        <li>Context pollution</li>
        <li>Poor performance</li>
      </ul>
    </div>
  </div>
  <div class="box warning" style="margin-top: 20px; font-size: 18px;">
    <strong>‚ö†Ô∏è Real Impact:</strong> A chatbot with 100K daily users can waste $1000s/day with poor context management
  </div>
</section>

<section>
  <h2>Context Pollution</h2>
  <h3>What is it?</h3>
  <p>Accumulation of irrelevant, redundant, or noisy information in the context window</p>
  <h3>Common Sources:</h3>
  <ul style="font-size: 20px;">
    <li><strong>Tool Spam</strong>: Failed attempts, debugging info</li>
    <li><strong>Duplicate Data</strong>: Repeated retrievals</li>
    <li><strong>Stale Information</strong>: Outdated context</li>
    <li><strong>Verbose Outputs</strong>: Unfiltered tool responses</li>
  </ul>
  <div class="box info" style="font-size: 18px;">
    <strong>Result:</strong> LLM gets "confused" ‚Üí hallucinations, incorrect responses, degraded performance
  </div>
</section>

<section>
  <h2>Multi-Agent Challenges</h2>
  <h3>Unique Context Issues:</h3>
  <ol style="font-size: 20px;">
    <li><strong>Shared vs. Isolated Context</strong> - Which agents see what?</li>
    <li><strong>Context Synchronization</strong> - Keeping agent states aligned</li>
    <li><strong>Exponential Growth</strong> - 3 agents √ó 10 messages = 30 message history</li>
    <li><strong>Handoff Complexity</strong> - What context to pass between agents?</li>
  </ol>
  <div class="box warning" style="margin-top: 20px;">
    <strong>Context Engineering is not optional for multi-agent apps‚Äîit's foundational</strong>
  </div>
</section>

<section>
  <h1 class="center">6 Optimization Techniques</h1>
  <p class="center" style="font-size: 28px; margin-top: 60px; color: #7f8c8d;">
    Essential Strategies for Context Management
  </p>
</section>

<section>
  <h2>1. Context Pruning</h2>
  <p><strong>Strategy:</strong> Remove oldest or least relevant messages</p>
<pre><code><span class=>import</span> { <span class="function">createAgent</span>, <span class="function">contextEditingMiddleware</span>,
         <span class="class">ClearToolUsesEdit</span> } <span class=>from</span> <span class="string">"langchain"</span>;

<span class=>const</span> <span class="variable">agent</span> = <span class="function">createAgent</span>({
  <span class="property">model</span>: <span class="string">"gpt-4"</span>,
  <span class="property">tools</span>: [<span class="variable">searchTool</span>, <span class="variable">calculatorTool</span>],
  <span class="property">middleware</span>: [
    <span class="function">contextEditingMiddleware</span>({
      <span class="property">edits</span>: [
        <span class=>new</span> <span class="class">ClearToolUsesEdit</span>({ 
          <span class="property">maxTokens</span>: <span class="number">1000</span>
        })
      ]
    })
  ]
});</code></pre>
  <div class="two-col" style="font-size: 18px;">
    <div>
      <p class="pros">‚úì Pros</p>
      <ul><li>Simple</li><li>Prevents overflow</li><li>Low overhead</li></ul>
    </div>
    <div>
      <p class="cons">‚úó Cons</p>
      <ul><li>May lose important info</li><li>No semantic awareness</li></ul>
    </div>
  </div>
</section>

<section>
  <h2>2. Summarization</h2>
  <p><strong>Strategy:</strong> Compress old messages into summaries</p>
<pre><code><span class=<span class="string"></span>>import</span> { createAgent, SummarizationMiddleware } 
<span class=<span class="string"></span>>from</span> <span class="string">"langchain"</span>;

<span class=<span class="string"></span>>const</span> agent = <span class="function">createAgent</span>({
  <span class="property">model</span>: <span class="string">"claude-sonnet-<span class="number">4</span>-<span class="number">5</span>-<span class="number">20250929</span>"</span>,
  <span class="property">tools</span>: [...],
  <span class="property">middleware</span>: [
    <span class=<span class="string"></span>>new</span> <span class="function">SummarizationMiddleware</span>({
      <span class="property">maxTokens</span>: <span class="number">2000</span>,
      <span class="property">summaryModel</span>: <span class="string">"gpt-4o"</span>,
      <span class="property">keepRecentMessages</span>: <span class="number">5</span>
    })
  ]
});</code></pre>
  <div class="two-col" style="font-size: 18px;">
    <div>
      <p class="pros">‚úì Pros</p>
      <ul><li>Preserves key info</li><li>Maintains continuity</li></ul>
    </div>
    <div>
      <p class="cons">‚úó Cons</p>
      <ul><li>Extra LLM call cost</li><li>Latency overhead</li></ul>
    </div>
  </div>
</section>

<section>
  <h2>3. Context Offloading</h2>
  <p><strong>Strategy:</strong> Store context in external memory</p>
<pre><code><span class="comment">// <span class="property">LangGraph</span>: Two types of memory</span>
<span class=<span class="string"></span>>import</span> { MemorySaver, InMemoryStore } <span class=<span class="string"></span>>from</span> <span class="string">"@langchain/langgraph"</span>;

<span class="comment">// <span class="number">1</span>. <span class="property">Checkpointer</span>: Thread-<span class="function">level</span>(short-term)</span>
<span class=<span class="string"></span>>const</span> checkpointer = <span class=<span class="string"></span>>new</span> <span class="function">MemorySaver</span>();

<span class="comment">// <span class="number">2</span>. <span class="property">Store</span>: Cross-<span class="function">thread</span>(long-term)  </span>
<span class=<span class="string"></span>>const</span> store = <span class=<span class="string"></span>>new</span> <span class="function">InMemoryStore</span>();

<span class=<span class="string"></span>>const</span> graph = <span class=<span class="string"></span>>new</span> <span class="function">StateGraph</span>({
  <span class="property">channels</span>: {
    <span class="property">messages</span>: { <span class="property">value</span>: (x, y) => x.<span class="function">concat</span>(y) }
  }
}).<span class="function">addNode</span>(<span class="string">"agent"</span>, agentNode);

<span class=<span class="string"></span>>const</span> app = graph.<span class="function">compile</span>({ 
  checkpointer,  <span class="comment">// Within conversation</span>
  store          <span class="comment">// Across conversations</span>
});

<span class=<span class="string"></span>>await</span> app.<span class="function">invoke</span>(input, { 
  <span class="property">configurable</span>: { 
    <span class="property">thread_id</span>: <span class="string">"conv-<span class="number">123</span>"</span>,
    <span class="property">user_id</span>: <span class="string">"user-<span class="number">456</span>"</span>  <span class="comment">// For store lookups</span>
  }
});</code></pre>
  <div class="two-col" style="font-size: 18px;">
    <div>
      <p class="pros">‚úì Pros</p>
      <ul><li>Infinite storage</li><li>Multi-session support</li></ul>
    </div>
    <div>
      <p class="cons">‚úó Cons</p>
      <ul><li>Infrastructure needed</li><li>Retrieval complexity</li></ul>
    </div>
  </div>
</section>

<section>
  <h2>4. Tool Offloading</h2>
  <p><strong>Strategy:</strong> Dynamically select which tools to expose</p>
<pre><code><span class=<span class="string"></span>>import</span> { createAgent, llmToolSelectorMiddleware } 
<span class=<span class="string"></span>>from</span> <span class="string">"langchain"</span>;

<span class=<span class="string"></span>>const</span> agent = <span class="function">createAgent</span>({
  <span class="property">model</span>: <span class="string">"gpt-4o"</span>,
  <span class="property">tools</span>: [simpleSearch, advancedSearch, 
          dataAnalysis, sqlQuery],
  <span class="property">middleware</span>: [
    <span class="function">llmToolSelectorMiddleware</span>({
      <span class="property">model</span>: <span class="string">"gpt-4o-mini"</span>,
      <span class="property">maxTools</span>: <span class="number">3</span>,
      <span class="property">alwaysInclude</span>: [<span class="string">"search"</span>]
    })
  ]
});</code></pre>
  <div class="two-col" style="font-size: 18px;">
    <div>
      <p class="pros">‚úì Pros</p>
      <ul><li>Reduces clutter</li><li>Faster reasoning</li></ul>
    </div>
    <div>
      <p class="cons">‚úó Cons</p>
      <ul><li>May miss needed tools</li><li>Selection overhead</li></ul>
    </div>
  </div>
</section>

<section>
  <h2>5. Semantic Editing</h2>
  <p><strong>Strategy:</strong> Use embeddings to filter irrelevant info</p>
<pre><code><span class=<span class="string"></span>>import</span> { OpenAIEmbeddings } <span class=<span class="string"></span>>from</span> <span class="string">"@langchain/openai"</span>;

<span class=<span class="string"></span>>async</span> <span class=<span class="string"></span>>function</span> <span class="function">semanticFilter</span>(
  <span class="property">messages</span>: Message[], 
  <span class="property">query</span>: string
) {
  <span class=<span class="string"></span>>const</span> embeddings = <span class=<span class="string"></span>>new</span> <span class="function">OpenAIEmbeddings</span>();
  <span class=<span class="string"></span>>const</span> queryEmbed = <span class=<span class="string"></span>>await</span> embeddings
    .<span class="function">embedQuery</span>(query);
  <span class=<span class="string"></span>>const</span> msgEmbeds = <span class=<span class="string"></span>>await</span> embeddings
    .<span class="function">embedDocuments</span>(messages.<span class="function">map</span>(m => m.content));
  
  <span class=<span class="string"></span>>const</span> scored = messages.<span class="function">map</span>((msg, i) => ({
    <span class="property">message</span>: msg,
    <span class="property">score</span>: <span class="function">cosineSimilarity</span>(queryEmbed, msgEmbeds[i])
  }));
  
  <span class=<span class="string"></span>>return</span> scored.<span class="function">filter</span>(s => s.score > <span class="number">0</span>.<span class="number">7</span>)
    .<span class="function">map</span>(s => s.message);
}</code></pre>
  <div class="two-col" style="font-size: 18px;">
    <div>
      <p class="pros">‚úì Pros</p>
      <ul><li>Intelligent filtering</li><li>Quality boost</li></ul>
    </div>
    <div>
      <p class="cons">‚úó Cons</p>
      <ul><li>Embedding costs</li><li>Complex to tune</li></ul>
    </div>
  </div>
</section>

<section>
  <h2>6. Hierarchical Context</h2>
  <p><strong>Strategy:</strong> Structure context in layers</p>
<pre><code><span class=<span class="string"></span>>interface</span> ContextHierarchy {
  <span class="property">global</span>: {
    <span class="property">systemPrompt</span>: string;
    <span class="property">userProfile</span>: UserProfile;
  };
  <span class="property">session</span>: {
    <span class="property">threadId</span>: string;
    <span class="property">summary</span>: string;
  };
  <span class="property">turn</span>: {
    <span class="property">recentMessages</span>: Message[];
    <span class="property">currentTools</span>: Tool[];
  };
}

<span class=<span class="string"></span>>const</span> constructContext = (<span class="property">h</span>: ContextHierarchy) => {
  <span class=<span class="string"></span>>return</span> [
    h.global.systemPrompt,
    `<span class="property">Summary</span>: ${h.session.summary}`,
    ...h.turn.recentMessages
  ];
};</code></pre>
  <div class="two-col" style="font-size: 18px;">
    <div>
      <p class="pros">‚úì Pros</p>
      <ul><li>Organized</li><li>Easy to debug</li></ul>
    </div>
    <div>
      <p class="cons">‚úó Cons</p>
      <ul><li>Complex architecture</li><li>Design upfront</li></ul>
    </div>
  </div>
</section>

<section>
  <h2>MCP & Context Optimization</h2>
  <p><strong>Model Context Protocol (MCP)</strong> - Standardized protocol for connecting AI to tools/data</p>
  
  <h3>How MCP Helps Context Optimization:</h3>
  <ol style="font-size: 20px;">
    <li><strong>On-Demand Tool Loading</strong>
      <ul style="font-size: 18px;">
        <li>Load tool definitions only when needed</li>
        <li>Reduces upfront context by 24% (tool definitions)</li>
      </ul>
    </li>
    <li><strong>Code Execution Pattern</strong>
      <ul style="font-size: 18px;">
        <li>Execute tools via code, not direct calls</li>
        <li><strong>Up to 98.7% token reduction</strong></li>
        <li>Intermediate results don't pass through LLM</li>
      </ul>
    </li>
    <li><strong>Smart Tool Discovery</strong>
      <ul style="font-size: 18px;">
        <li><code>search_tools()</code> - find relevant tools dynamically</li>
        <li>Tiered detail levels (name only ‚Üí full schema)</li>
      </ul>
    </li>
  </ol>
</section>

<section>
  <h2>MCP Context Optimization Example</h2>
  
  <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; font-size: 18px;">
    <div>
      <h3 style="font-size: 24px; color: #e74c3c;">‚ùå Traditional MCP</h3>
<pre style="font-size: 13px;"><code>// All 50 tool definitions loaded
// ~20k tokens just for tools!

TOOL CALL: gdrive.getDocument()
‚Üí 50k token transcript

TOOL CALL: salesforce.update()
‚Üí Copy entire 50k tokens again

Total: ~120k tokens</code></pre>
    </div>
    
    <div>
      <h3 style="font-size: 24px; color: #27ae60;">‚úì Code Execution + MCP</h3>
<pre style="font-size: 13px;"><code>// Load only needed tools on-demand
// ~500 tokens for 2 tools

const doc = await getDocument();
// Process locally in sandbox
const summary = summarize(doc);
await updateSalesforce(summary);

Total: ~2k tokens (98% reduction!)</code></pre>
    </div>
  </div>
  
  <div class="box success" style="margin-top: 20px; font-size: 18px;">
    <strong>Key Insight:</strong> MCP + Code Execution = Best of both worlds - 
    standardized integrations + minimal context pollution
  </div>
</section>

<section>
  <h2>MCP Best Practices for Context</h2>
  
  <ol style="font-size: 22px;">
    <li><strong>Optimize Tool Descriptions</strong>
      <ul style="font-size: 20px;">
        <li>Keep descriptions minimal but clear</li>
        <li>Every character counts in tool schemas</li>
      </ul>
    </li>
    <li><strong>Filter API Responses</strong>
      <ul style="font-size: 20px;">
        <li>Return only what the model needs</li>
        <li>Example: 65.4% reduction removing 12 unused fields</li>
      </ul>
    </li>
    <li><strong>Aggregate Pre-emptively</strong>
      <ul style="font-size: 20px;">
        <li>Compute summaries server-side</li>
        <li>Example: 94.6% reduction for 6-month spending query</li>
      </ul>
    </li>
    <li><strong>Use Code Execution</strong>
      <ul style="font-size: 20px;">
        <li>Process data in sandboxed environment</li>
        <li>Only return final results to LLM</li>
      </ul>
    </li>
  </ol>
</section>

<section>
  <h2>MCP + LangChain Integration</h2>
  
<pre style="font-size: 16px;"><code>// LangChain MCP integration (TypeScript)
import { load_mcp_tools } from "@langchain/mcp-adapters";
import { MultiServerMCPClient } from "@langchain/mcp";

// Connect to MCP servers
const client = new MultiServerMCPClient({
  servers: {
    github: { url: "http://localhost:3000" },
    notion: { url: "http://localhost:3001" }
  }
});

// Load tools with session management
const session = await client.session("github");
const tools = await load_mcp_tools(session);

// Use with LangChain agent
const agent = createAgent({
  model: "gpt-4o",
  tools: tools,  // MCP tools automatically optimized
  middleware: [
    // Combine with other context optimizations
    new SummarizationMiddleware({ maxTokens: 2000 })
  ]
});
</code></pre>
  
  <p style="font-size: 18px; margin-top: 15px;">
    <strong>Benefit:</strong> MCP handles tool connectivity, LangChain handles context management
  </p>
</section>

<section>
  <h2>Technique Comparison</h2>
  <table>
    <thead>
      <tr>
        <th>Technique</th>
        <th>Complexity</th>
        <th>Cost</th>
        <th>Best For</th>
      </tr>
    </thead>
    <tbody>
      <tr><td>Pruning</td><td>Low</td><td>Low</td><td>Simple bots</td></tr>
      <tr><td>Summarization</td><td>Medium</td><td>Medium</td><td>Long conversations</td></tr>
      <tr><td>Context Offload</td><td>Medium</td><td>Low</td><td>Multi-session</td></tr>
      <tr><td>Tool Offload</td><td>Low</td><td>Low</td><td>Many tools</td></tr>
      <tr><td>Semantic Editing</td><td>High</td><td>Medium</td><td>Quality-critical</td></tr>
      <tr><td>Hierarchical</td><td>High</td><td>Low</td><td>Enterprise</td></tr>
      <tr><td><strong>MCP + Code Exec</strong></td><td>Medium</td><td><strong>Very Low</strong></td><td>Tool-heavy agents</td></tr>
    </tbody>
  </table>
</section>

<section>
  <h2>Multi-Agent Patterns</h2>
  <h3>Design Patterns:</h3>
  <ol style="font-size: 20px;">
    <li><strong>Isolated Contexts</strong>
      <ul style="font-size: 18px;">
        <li>Each agent maintains separate state</li>
        <li>Explicit handoff with context summary</li>
      </ul>
    </li>
    <li><strong>Shared Memory Pool</strong>
      <ul style="font-size: 18px;">
        <li>Central memory all agents query</li>
        <li>Selective retrieval per agent</li>
      </ul>
    </li>
    <li><strong>Supervisor Pattern</strong>
      <ul style="font-size: 18px;">
        <li>Coordinator manages context distribution</li>
        <li>Workers get task-specific context</li>
      </ul>
    </li>
  </ol>
  <div class="box info" style="font-size: 16px;">
    <strong>LangGraph Tip:</strong> Use channels with custom reducers to control state merging
  </div>
</section>

<section>
  <h2>Production Example</h2>
<pre><code><span class="comment">// Customer support with context optimization</span>
<span class=<span class="string"></span>>import</span> { createAgent, SummarizationMiddleware } 
<span class=<span class="string"></span>>from</span> <span class="string">"langchain"</span>;

<span class=<span class="string"></span>>const</span> triageAgent = <span class="function">createAgent</span>({
  <span class="property">model</span>: <span class="string">"gpt-4o"</span>,
  <span class="property">tools</span>: [searchKB, checkStatus],
  <span class="property">middleware</span>: [
    <span class="function">llmToolSelectorMiddleware</span>({ <span class="property">maxTools</span>: <span class="number">2</span> }),
    <span class=<span class="string"></span>>new</span> <span class="function">SummarizationMiddleware</span>({ <span class="property">maxTokens</span>: <span class="number">1500</span> })
  ]
});

<span class=<span class="string"></span>>const</span> specialistAgent = <span class="function">createAgent</span>({
  <span class="property">model</span>: <span class="string">"gpt-4o"</span>,
  <span class="property">tools</span>: [refundTool, escalateTool],
  <span class="property">middleware</span>: [
    <span class=<span class="string"></span>>new</span> <span class="function">SummarizationMiddleware</span>({ 
      <span class="property">keepRecentMessages</span>: <span class="number">3</span>
    })
  ]
});</code></pre>
  <div class="box success" style="font-size: 18px;">
    <strong>Result:</strong> 60% cost reduction, 40% faster responses
  </div>
</section>

<section>
  <h2>Best Practices</h2>
  <ol style="font-size: 22px;">
    <li><strong>Monitor Token Usage</strong> - Use LangSmith for observability</li>
    <li><strong>Start Simple</strong> - Pruning ‚Üí Summarization ‚Üí Advanced</li>
    <li><strong>Test Strategies</strong> - A/B test different approaches</li>
    <li><strong>Combine Techniques</strong> - Tool offload + Summarization works well</li>
    <li><strong>Design for Failure</strong> - Graceful degradation</li>
  </ol>
</section>

<section>
  <h2>Common Pitfalls</h2>
  <div class="box warning">
    <h3>‚ö†Ô∏è Watch Out For:</h3>
    <ul style="font-size: 20px;">
      <li><strong>Over-aggressive pruning</strong> - Breaks continuity</li>
      <li><strong>Ignoring tool bloat</strong> - Tool calls dominate context</li>
      <li><strong>No fallback strategy</strong> - What if summarization fails?</li>
      <li><strong>Premature optimization</strong> - Measure first</li>
      <li><strong>Forgetting multi-modal</strong> - Images consume tokens too</li>
    </ul>
  </div>
</section>

<section>
  <h2>Tools & Resources</h2>
  <h3>LangChain Ecosystem (v1.0+)</h3>
  <ul style="font-size: 20px;">
    <li><strong>LangChain</strong>: Agent framework with middleware</li>
    <li><strong>LangGraph</strong>: Stateful multi-agent orchestration</li>
    <li><strong>LangSmith</strong>: Observability & debugging</li>
  </ul>
  <h3>Key TypeScript Packages</h3>
  <ul style="font-size: 20px;">
    <li><code>langchain</code> - Core agent creation</li>
    <li><code>@langchain/langgraph</code> - Graph workflows</li>
    <li><code>@langchain/openai</code> - OpenAI integration</li>
  </ul>
</section>

<section>
  <h2>Key Takeaways</h2>
  <ol style="font-size: 24px;">
    <li>Context is the <strong>working memory</strong> of LLMs</li>
    <li>Poor management = high costs + slow + errors</li>
    <li>Context pollution degrades performance</li>
    <li>Six techniques available, each with tradeoffs</li>
    <li>Multi-agent systems require context engineering</li>
    <li>LangChain 1.0 has built-in middleware</li>
    <li><strong>Monitor, measure, then optimize</strong></li>
  </ol>
</section>

<section>
  <h1 class="center">Questions?</h1>
  <p class="center" style="font-size: 28px; margin-top: 60px;">
    Let's discuss your context optimization challenges
  </p>
  <div class="center" style="margin-top: 80px; font-size: 20px; color: #7f8c8d;">
    <p><strong>Resources:</strong></p>
    <p style="margin-top: 20px;">docs.langchain.com</p>
    <p>github.com/langchain-ai/langgraph</p>
    <p>smith.langchain.com</p>
  </div>
</section>

<div id="progress"></div>
<div id="slide-number"></div>

<script>
var Presentation = {
  slides: null,
  currentSlide: 0,
  
  init: function() {
    this.slides = document.querySelectorAll('section');
    this.setSlide(0);
    this.updateProgress();
    
    window.addEventListener('keydown', function(e) {
      if (e.key === 'ArrowRight' || e.key === 'PageDown' || e.key === ' ') {
        e.preventDefault();
        Presentation.next();
      } else if (e.key === 'ArrowLeft' || e.key === 'PageUp') {
        e.preventDefault();
        Presentation.prev();
      } else if (e.key === 'Home') {
        e.preventDefault();
        Presentation.goTo(0);
      } else if (e.key === 'End') {
        e.preventDefault();
        Presentation.goTo(Presentation.slides.length - 1);
      }
    });
    
    document.body.addEventListener('click', function(e) {
      if (e.target.tagName !== 'A' && e.target.tagName !== 'CODE') {
        if (e.clientX > window.innerWidth / 2) {
          Presentation.next();
        } else {
          Presentation.prev();
        }
      }
    });
  },
  
  setSlide: function(n) {
    if (n < 0 || n >= this.slides.length) return;
    
    for (var i = 0; i < this.slides.length; i++) {
      this.slides[i].removeAttribute('aria-selected');
    }
    
    this.slides[n].setAttribute('aria-selected', 'true');
    this.currentSlide = n;
    this.updateProgress();
    this.updateSlideNumber();
  },
  
  next: function() {
    if (this.currentSlide < this.slides.length - 1) {
      this.setSlide(this.currentSlide + 1);
    }
  },
  
  prev: function() {
    if (this.currentSlide > 0) {
      this.setSlide(this.currentSlide - 1);
    }
  },
  
  goTo: function(n) {
    this.setSlide(n);
  },
  
  updateProgress: function() {
    var progress = document.getElementById('progress');
    var percent = (this.currentSlide / (this.slides.length - 1)) * 100;
    progress.style.width = percent + '%';
  },
  
  updateSlideNumber: function() {
    var slideNum = document.getElementById('slide-number');
    slideNum.textContent = (this.currentSlide + 1) + ' / ' + this.slides.length;
  }
};

window.addEventListener('load', function() {
  Presentation.init();
});
</script>

</body>
</html>
